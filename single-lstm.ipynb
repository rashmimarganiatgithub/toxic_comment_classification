{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from contextlib import contextmanager\n",
    "import copy\n",
    "from functools import partial\n",
    "from itertools import chain\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, Sampler, DataLoader\n",
    "from torch.optim.optimizer import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FASTTEXT = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n",
    "TRAIN_DATA = '../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv'\n",
    "TEST_DATA = '../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv'\n",
    "SAMPLE_SUBMISSION = '../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv'\n",
    "\n",
    "embed_size = 300\n",
    "max_features = 100000\n",
    "max_len = 220\n",
    "\n",
    "batch_size = 512\n",
    "train_epochs = 6\n",
    "n_splits = 5\n",
    "\n",
    "mu = 0.9\n",
    "updates_per_epoch = 10\n",
    "\n",
    "seed = 1029\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "lc = LancasterStemmer()\n",
    "sb = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(msg):\n",
    "    t0 = time.time()\n",
    "    print(f'[{msg}] start.')\n",
    "    yield\n",
    "    elapsed_time = time.time() - t0\n",
    "    print(f'[{msg}] done in {elapsed_time / 60:.2f} min.')\n",
    "\n",
    "\n",
    "def seed_torch(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspell_dict = {\"aren't\": \"are not\", \"can't\": \"cannot\", \"couldn't\": \"could not\",\n",
    "                 \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\",\n",
    "                 \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                 \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\",\n",
    "                 \"i'd\": \"I had\", \"i'll\": \"I will\", \"i'm\": \"I am\", \"isn't\": \"is not\",\n",
    "                 \"it's\": \"it is\", \"it'll\": \"it will\", \"i've\": \"I have\", \"let's\": \"let us\",\n",
    "                 \"mightn't\": \"might not\", \"mustn't\": \"must not\", \"shan't\": \"shall not\",\n",
    "                 \"she'd\": \"she would\", \"she'll\": \"she will\", \"she's\": \"she is\",\n",
    "                 \"shouldn't\": \"should not\", \"that's\": \"that is\", \"there's\": \"there is\",\n",
    "                 \"they'd\": \"they would\", \"they'll\": \"they will\", \"they're\": \"they are\",\n",
    "                 \"they've\": \"they have\", \"we'd\": \"we would\", \"we're\": \"we are\",\n",
    "                 \"weren't\": \"were not\", \"we've\": \"we have\", \"what'll\": \"what will\",\n",
    "                 \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\",\n",
    "                 \"where's\": \"where is\", \"who'd\": \"who would\", \"who'll\": \"who will\",\n",
    "                 \"who're\": \"who are\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                 \"won't\": \"will not\", \"wouldn't\": \"would not\", \"you'd\": \"you would\",\n",
    "                 \"you'll\": \"you will\", \"you're\": \"you are\", \"you've\": \"you have\",\n",
    "                 \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\": \" will\", \"tryin'\": \"trying\"}\n",
    "\n",
    "\n",
    "def _get_misspell(misspell_dict):\n",
    "    misspell_re = re.compile('(%s)' % '|'.join(misspell_dict.keys()))\n",
    "    return misspell_dict, misspell_re\n",
    "\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    misspellings, misspellings_re = _get_misspell(misspell_dict)\n",
    "\n",
    "    def replace(match):\n",
    "        return misspellings[match.group(0)]\n",
    "\n",
    "    return misspellings_re.sub(replace, text)\n",
    "    \n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']',\n",
    "          '>', '%', '=', '#', '*', '+', '\\\\', '•', '~', '@', '£', '·', '_', '{', '}', '©', '^',\n",
    "          '®', '`', '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', 'Â', '█',\n",
    "          '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶',\n",
    "          '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼',\n",
    "          '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "          'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪',\n",
    "          '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√']\n",
    "\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts + list(string.punctuation):\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "\n",
    "def clean_numbers(x):\n",
    "    return re.sub(r'\\d+', ' ', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(embedding_path, word_index):\n",
    "\n",
    "    def get_coefs(word, *arr):\n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    embeddings_index = dict(get_coefs(*o.strip().split(' ')) for o in open(embedding_path))\n",
    "    \n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features + 2, len(word_index))\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "\n",
    "    for key, i in word_index.items():\n",
    "        word = key\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        word = key.lower()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        word = key.upper()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        word = key.capitalize()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        word = ps.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        word = lc.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        word = sb.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prec():\n",
    "    train = pd.read_csv(TRAIN_DATA, index_col='id')\n",
    "    test = pd.read_csv(TEST_DATA, index_col='id')\n",
    "    \n",
    "    # lower\n",
    "    train['comment_text'] = train['comment_text'].str.lower()\n",
    "    test['comment_text'] = test['comment_text'].str.lower()\n",
    "\n",
    "    # clean misspellings\n",
    "    train['comment_text'] = train['comment_text'].apply(replace_typical_misspell)\n",
    "    test['comment_text'] = test['comment_text'].apply(replace_typical_misspell)\n",
    "\n",
    "    # clean the text\n",
    "    train['comment_text'] = train['comment_text'].apply(clean_text)\n",
    "    test['comment_text'] = test['comment_text'].apply(clean_text)\n",
    "\n",
    "    # clean numbers\n",
    "    train['comment_text'] = train['comment_text'].apply(clean_numbers)\n",
    "    test['comment_text'] = test['comment_text'].apply(clean_numbers)\n",
    "    \n",
    "    # strip\n",
    "    train['comment_text'] = train['comment_text'].str.strip()\n",
    "    test['comment_text'] = test['comment_text'].str.strip()\n",
    "    \n",
    "    # replace blank with nan\n",
    "    train['comment_text'].replace('', np.nan, inplace=True)\n",
    "    test['comment_text'].replace('', np.nan, inplace=True)\n",
    "\n",
    "    # nan prediction\n",
    "    nan_pred = train['target'][train['comment_text'].isna()].mean()\n",
    "    \n",
    "    # fill up the missing values\n",
    "    train_x = train['comment_text'].fillna('_##_').values\n",
    "    test_x = test['comment_text'].fillna('_##_').values\n",
    "    \n",
    "    # get the target values\n",
    "    identity_columns = [\n",
    "        'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "        'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "\n",
    "    weights = np.ones((len(train),))\n",
    "    weights += train[identity_columns].fillna(0).values.sum(axis=1) * 3\n",
    "    weights += train['target'].values * 8\n",
    "    weights /= weights.max()\n",
    "    train_y = np.vstack([train['target'].values, weights]).T\n",
    "    \n",
    "    train_y_identity = train[identity_columns].values\n",
    "\n",
    "    # shuffling the data\n",
    "    np.random.seed(seed)\n",
    "    train_idx = np.random.permutation(len(train_x))\n",
    "\n",
    "    train_x = train_x[train_idx]\n",
    "    train_y = train_y[train_idx]\n",
    "    train_y_identity = train_y_identity[train_idx]\n",
    "\n",
    "    return train_x, train_y, train_y_identity, test_x, nan_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts, max_features):\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        counter.update(text.split())\n",
    "\n",
    "    vocab = {\n",
    "        'token2id': {'<PAD>': 0, '<UNK>': max_features + 1},\n",
    "        'id2token': {}\n",
    "    }\n",
    "    vocab['token2id'].update(\n",
    "        {token: _id + 1 for _id, (token, count) in\n",
    "         enumerate(counter.most_common(max_features))})\n",
    "    vocab['id2token'] = {v: k for k, v in vocab['token2id'].items()}\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def tokenize(texts, vocab):\n",
    "    \n",
    "    def text2ids(text, token2id):\n",
    "        return [\n",
    "            token2id.get(token, len(token2id) - 1)\n",
    "            for token in text.split()[:max_len]]\n",
    "    \n",
    "    return [\n",
    "        text2ids(text, vocab['token2id'])\n",
    "        for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: [5th place solution](https://www.kaggle.com/jiangm/5th-place-solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, seqs, targets=None, maxlen=200):\n",
    "        if targets is not None:\n",
    "            self.targets = targets\n",
    "        else:\n",
    "            self.targets = np.random.randint(2, size=(len(seqs),))\n",
    "        \n",
    "        self.seqs = seqs\n",
    "        self.maxlen = maxlen\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "        \n",
    "    def get_keys(self):\n",
    "        lens = np.fromiter(\n",
    "            ((min(self.maxlen, len(seq))) for seq in self.seqs),\n",
    "            dtype=np.int32)\n",
    "        return lens\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return index, self.seqs[index], self.targets[index]\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "\n",
    "    def _pad_sequences(seqs):\n",
    "        lens = [len(seq) for seq in seqs]\n",
    "        max_len = max(lens)\n",
    "\n",
    "        padded_seqs = torch.zeros(len(seqs), max_len).long()\n",
    "        for i, seq in enumerate(seqs):\n",
    "            start = max_len - lens[i]\n",
    "            padded_seqs[i, start:] = torch.LongTensor(seq)\n",
    "        return padded_seqs\n",
    "\n",
    "    index, seqs, targets = zip(*data)\n",
    "    seqs = _pad_sequences(seqs)\n",
    "    return index, seqs, torch.FloatTensor(targets)\n",
    "\n",
    "\n",
    "class BucketSampler(Sampler):\n",
    "\n",
    "    def __init__(self, data_source, sort_keys, bucket_size=None, batch_size=1048, shuffle_data=True):\n",
    "        super().__init__(data_source)\n",
    "        self.shuffle = shuffle_data\n",
    "        self.batch_size = batch_size\n",
    "        self.sort_keys = sort_keys\n",
    "        self.bucket_size = bucket_size if bucket_size is not None else len(sort_keys)\n",
    "        self.weights = None\n",
    "\n",
    "        if not shuffle_data:\n",
    "            self.index = self.prepare_buckets()\n",
    "        else:\n",
    "            self.index = None\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        assert weights >= 0\n",
    "        total = np.sum(weights)\n",
    "        if total != 1:\n",
    "            weights = weights / total\n",
    "        self.weights = weights\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = None\n",
    "        if self.weights is not None:\n",
    "            total = len(self.sort_keys)\n",
    "            indices = np.random.choice(total, (total,), p=self.weights)\n",
    "        if self.shuffle:\n",
    "            self.index = self.prepare_buckets(indices)\n",
    "        return iter(self.index)\n",
    "\n",
    "    def get_reverse_indexes(self):\n",
    "        indexes = np.zeros((len(self.index),), dtype=np.int32)\n",
    "        for i, j in enumerate(self.index):\n",
    "            indexes[j] = i\n",
    "        return indexes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sort_keys)\n",
    "        \n",
    "    def prepare_buckets(self, indices=None):\n",
    "        lens = - self.sort_keys\n",
    "        assert self.bucket_size % self.batch_size == 0 or self.bucket_size == len(lens)\n",
    "\n",
    "        if indices is None:\n",
    "            if self.shuffle:\n",
    "                indices = shuffle(np.arange(len(lens), dtype=np.int32))\n",
    "                lens = lens[indices]\n",
    "            else:\n",
    "                indices = np.arange(len(lens), dtype=np.int32)\n",
    "\n",
    "        #  bucket iterator\n",
    "        def divide_chunks(l, n):\n",
    "            if n == len(l):\n",
    "                yield np.arange(len(l), dtype=np.int32), l\n",
    "            else:\n",
    "                # looping till length l\n",
    "                for i in range(0, len(l), n):\n",
    "                    data = l[i:i + n]\n",
    "                    yield np.arange(i, i + len(data), dtype=np.int32), data\n",
    "    \n",
    "        new_indices = []\n",
    "        extra_batch = None\n",
    "        for chunk_index, chunk in divide_chunks(lens, self.bucket_size):\n",
    "            # sort indices in bucket by descending order of length\n",
    "            indices_sorted = chunk_index[np.argsort(chunk, axis=-1)]\n",
    "            batches = []\n",
    "            for _, batch in divide_chunks(indices_sorted, self.batch_size):\n",
    "                if len(batch) == self.batch_size:\n",
    "                    batches.append(batch.tolist())\n",
    "                else:\n",
    "                    assert extra_batch is None\n",
    "                    assert batch is not None\n",
    "                    extra_batch = batch\n",
    "    \n",
    "            # shuffling batches within buckets\n",
    "            if self.shuffle:\n",
    "                batches = shuffle(batches)\n",
    "            for batch in batches:\n",
    "                new_indices.extend(batch)\n",
    "    \n",
    "        if extra_batch is not None:\n",
    "            new_indices.extend(extra_batch)\n",
    "        return indices[new_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        lstm_hidden_size = 120\n",
    "        gru_hidden_size = 60\n",
    "        self.gru_hidden_size = gru_hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(*embedding_matrix.shape)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = nn.Dropout2d(0.2)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_matrix.shape[1], lstm_hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.gru = nn.GRU(lstm_hidden_size * 2, gru_hidden_size, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.linear = nn.Linear(gru_hidden_size * 6, 20)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.Linear(20, 1)\n",
    "        \n",
    "    def apply_spatial_dropout(self, h_embedding):\n",
    "        h_embedding = h_embedding.transpose(1, 2).unsqueeze(2)\n",
    "        h_embedding = self.embedding_dropout(h_embedding).squeeze(2).transpose(1, 2)\n",
    "        return h_embedding\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = self.apply_spatial_dropout(h_embedding)\n",
    "\n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        h_gru, hh_gru = self.gru(h_lstm)\n",
    "\n",
    "        hh_gru = hh_gru.view(-1, self.gru_hidden_size * 2)\n",
    "\n",
    "        avg_pool = torch.mean(h_gru, 1)\n",
    "        max_pool, _ = torch.max(h_gru, 1)\n",
    "\n",
    "        conc = torch.cat((hh_gru, avg_pool, max_pool), 1)\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        conc = self.dropout(conc)\n",
    "        out = self.out(conc)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: [PME_EMA 6 x 8 pochs](https://www.kaggle.com/tks0123456789/pme-ema-6-x-8-pochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA:\n",
    "\n",
    "    def __init__(self, model, mu, level='batch', n=1):\n",
    "        # self.ema_model = copy.deepcopy(model)\n",
    "        self.mu = mu\n",
    "        self.level = level\n",
    "        self.n = n\n",
    "        self.cnt = self.n\n",
    "        self.shadow = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data\n",
    "\n",
    "    def _update(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                new_average = (1 - self.mu) * param.data + self.mu * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def set_weights(self, ema_model):\n",
    "        for name, param in ema_model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def on_batch_end(self, model):\n",
    "        if self.level is 'batch':\n",
    "            self.cnt -= 1\n",
    "            if self.cnt == 0:\n",
    "                self._update(model)\n",
    "                self.cnt = self.n\n",
    "                \n",
    "    def on_epoch_end(self, model):\n",
    "        if self.level is 'epoch':\n",
    "            self._update(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamScheduler:\n",
    "    \n",
    "    def __init__(self, optimizer, scale_fn, step_size):\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.scale_fn = scale_fn\n",
    "        self.step_size = step_size\n",
    "        self.last_batch_iteration = 0\n",
    "        \n",
    "    def batch_step(self):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = self.scale_fn(self.last_batch_iteration / self.step_size)\n",
    "        \n",
    "        self.last_batch_iteration += 1\n",
    "\n",
    "\n",
    "def combine_scale_functions(scale_fns, phases=None):\n",
    "    if phases is None:\n",
    "        phases = [1. / len(scale_fns)] * len(scale_fns)\n",
    "    phases = [phase / sum(phases) for phase in phases]\n",
    "    phases = torch.tensor([0] + phases)\n",
    "    phases = torch.cumsum(phases, 0)\n",
    "    \n",
    "    def _inner(x):\n",
    "        idx = (x >= phases).nonzero().max()\n",
    "        actual_x = (x - phases[idx]) / (phases[idx + 1] - phases[idx])\n",
    "        return scale_fns[idx](actual_x)\n",
    "        \n",
    "    return _inner\n",
    "\n",
    "\n",
    "def scale_cos(start, end, x):\n",
    "    return start + (1 + np.cos(np.pi * (1 - x))) * (end - start) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawEvaluator:\n",
    "    \n",
    "    def __init__(self, y_binary, y_identity_binary, power=-5, overall_model_weight=0.25):\n",
    "        self.y = y_binary\n",
    "        self.y_i = y_identity_binary\n",
    "        self.n_subgroups = self.y_i.shape[1]\n",
    "        self.power = power\n",
    "        self.overall_model_weight = overall_model_weight\n",
    "        \n",
    "    @staticmethod\n",
    "    def _compute_auc(y_true, y_pred):\n",
    "        try:\n",
    "            return roc_auc_score(y_true, y_pred)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "        \n",
    "    def _compute_subgroup_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] == 1\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "        \n",
    "    def _compute_bpsn_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] + self.y == 1\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "        \n",
    "    def _compute_bnsp_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] + self.y != 1\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "        \n",
    "    def compute_bias_metrics_for_model(self, y_pred):\n",
    "        records = np.zeros((3, self.n_subgroups))\n",
    "        for i in range(self.n_subgroups):\n",
    "            records[0, i] = self._compute_subgroup_auc(i, y_pred)\n",
    "            records[1, i] = self._compute_bpsn_auc(i, y_pred)\n",
    "            records[2, i] = self._compute_bnsp_auc(i, y_pred)\n",
    "        return records\n",
    "        \n",
    "    def _calculate_overall_auc(self, y_pred):\n",
    "        return roc_auc_score(self.y, y_pred)\n",
    "        \n",
    "    def _power_mean(self, array):\n",
    "        total = sum(np.power(array, self.power))\n",
    "        return np.power(total / len(array), 1 / self.power)\n",
    "        \n",
    "    def get_final_metric(self, y_pred):\n",
    "        bias_metrics = self.compute_bias_metrics_for_model(y_pred)\n",
    "        bias_score = np.average([\n",
    "            self._power_mean(bias_metrics[0]),\n",
    "            self._power_mean(bias_metrics[1]),\n",
    "            self._power_mean(bias_metrics[2])\n",
    "        ])\n",
    "        overall_score = self.overall_model_weight * self._calculate_overall_auc(y_pred)\n",
    "        bias_score = (1 - self.overall_model_weight) * bias_score\n",
    "        return overall_score + bias_score\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    model.eval()\n",
    "    preds_fold = np.zeros(len(data_loader.dataset))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for index, x_batch, y_batch in data_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_pred = model(x_batch).detach()\n",
    "            preds_fold[list(index)] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "    return preds_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "seed_torch(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load data] start.\n",
      "[load data] done in 6.34 min.\n"
     ]
    }
   ],
   "source": [
    "with timer('load data'):\n",
    "    train_x, train_y, train_y_identity, test_x, nan_pred = load_and_prec()\n",
    "    train_nan_mask = train_x == '_##_'\n",
    "    test_nan_mask = test_x == '_##_'\n",
    "    y_binary = (train_y[:, 0] >= 0.5).astype(int)\n",
    "    y_identity_binary = (train_y_identity >= 0.5).astype(int)\n",
    "    vocab = build_vocab(chain(train_x, test_x), max_features)\n",
    "    embedding_matrix = load_embedding(EMBEDDING_FASTTEXT, vocab['token2id'])\n",
    "\n",
    "    train_x = np.array(tokenize(train_x, vocab))\n",
    "    test_x = np.array(tokenize(test_x, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pseudo label] start.\n",
      "Epoch 1/6 \t time=125.60s\n",
      "Epoch 2/6 \t time=126.21s\n",
      "Epoch 3/6 \t time=124.98s\n",
      "Epoch 4/6 \t time=126.37s\n",
      "Epoch 5/6 \t time=125.49s\n",
      "Epoch 6/6 \t time=125.27s\n",
      "[pseudo label] done in 13.09 min.\n"
     ]
    }
   ],
   "source": [
    "with timer('pseudo label'):\n",
    "    train_preds = np.zeros((len(train_x)))\n",
    "    test_preds = np.zeros((len(test_x)))\n",
    "\n",
    "    ema_train_preds = np.zeros((len(train_x)))\n",
    "    ema_test_preds = np.zeros((len(test_x)))\n",
    "\n",
    "    train_dataset = TextDataset(train_x, targets=train_y, maxlen=max_len)\n",
    "    test_dataset = TextDataset(test_x, maxlen=max_len)\n",
    "\n",
    "    train_sampler = BucketSampler(train_dataset, train_dataset.get_keys(),\n",
    "                                  bucket_size=batch_size * 20, batch_size=batch_size)\n",
    "    test_sampler = BucketSampler(test_dataset, test_dataset.get_keys(),\n",
    "                                 batch_size=batch_size, shuffle_data=False)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False,\n",
    "                              sampler=train_sampler, num_workers=0, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=test_sampler,\n",
    "                             shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "    models = {}\n",
    "    model = NeuralNet(embedding_matrix).to(device)\n",
    "\n",
    "    ema_model = copy.deepcopy(model)\n",
    "    ema_model.eval()\n",
    "\n",
    "    ema_n = int(len(train_loader.dataset) / (updates_per_epoch * batch_size))\n",
    "    ema = EMA(model, mu, n=ema_n)\n",
    "\n",
    "    scale_fn = combine_scale_functions(\n",
    "        [partial(scale_cos, 1e-4, 5e-3), partial(scale_cos, 5e-3, 1e-3)], [0.2, 0.8])\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    scheduler = ParamScheduler(optimizer, scale_fn, train_epochs * len(train_loader))\n",
    "\n",
    "    all_test_preds = []\n",
    "\n",
    "    for epoch in range(train_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "\n",
    "        for _, x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            scheduler.batch_step()\n",
    "            y_pred = model(x_batch)\n",
    "\n",
    "            loss = nn.BCEWithLogitsLoss(weight=y_batch[:, 1])(y_pred[:, 0], y_batch[:, 0])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ema.on_batch_end(model)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('Epoch {}/{} \\t time={:.2f}s'.format(\n",
    "            epoch + 1, train_epochs, elapsed_time))\n",
    "\n",
    "        test_preds = eval_model(model, test_loader)\n",
    "        all_test_preds.append(test_preds)\n",
    "\n",
    "        ema.on_epoch_end(model)\n",
    "\n",
    "    ema.set_weights(ema_model)\n",
    "    ema_model.lstm.flatten_parameters()\n",
    "    ema_model.gru.flatten_parameters()\n",
    "\n",
    "    checkpoint_weights = np.array([2 ** epoch for epoch in range(train_epochs)])\n",
    "    checkpoint_weights = checkpoint_weights / checkpoint_weights.sum()\n",
    "\n",
    "    ema_test_y = eval_model(ema_model, test_loader)\n",
    "    test_y = np.average(all_test_preds, weights=checkpoint_weights, axis=0)\n",
    "    test_y = np.mean([test_y, ema_test_y], axis=0)\n",
    "    test_y[test_nan_mask] = nan_pred\n",
    "    weight = np.ones((len(test_y)))\n",
    "    test_y = np.vstack((test_y, weight)).T\n",
    "\n",
    "    models['model'] = model.state_dict()\n",
    "    models['ema_model'] = ema_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] start.\n",
      "Fold 1\n",
      "Epoch 1/6 \t auc=0.92372 \t time=119.82s\n",
      "Epoch 2/6 \t auc=0.93374 \t time=120.59s\n",
      "Epoch 3/6 \t auc=0.93449 \t time=119.79s\n",
      "Epoch 4/6 \t auc=0.93626 \t time=118.54s\n",
      "Epoch 5/6 \t auc=0.93744 \t time=118.28s\n",
      "Epoch 6/6 \t auc=0.93748 \t time=119.23s\n",
      "cv model \t auc=0.93748\n",
      "EMA model \t auc=0.93807\n",
      "Fold 2\n",
      "Epoch 1/6 \t auc=0.92425 \t time=120.02s\n",
      "Epoch 2/6 \t auc=0.93042 \t time=119.14s\n",
      "Epoch 3/6 \t auc=0.93351 \t time=120.30s\n",
      "Epoch 4/6 \t auc=0.93412 \t time=120.64s\n",
      "Epoch 5/6 \t auc=0.93503 \t time=119.96s\n",
      "Epoch 6/6 \t auc=0.93543 \t time=119.20s\n",
      "cv model \t auc=0.93543\n",
      "EMA model \t auc=0.93585\n",
      "Fold 3\n",
      "Epoch 1/6 \t auc=0.92365 \t time=120.01s\n",
      "Epoch 2/6 \t auc=0.92942 \t time=119.42s\n",
      "Epoch 3/6 \t auc=0.93215 \t time=119.29s\n",
      "Epoch 4/6 \t auc=0.93376 \t time=121.17s\n",
      "Epoch 5/6 \t auc=0.93362 \t time=120.10s\n",
      "Epoch 6/6 \t auc=0.93444 \t time=119.79s\n",
      "cv model \t auc=0.93444\n",
      "EMA model \t auc=0.93465\n",
      "Fold 4\n",
      "Epoch 1/6 \t auc=0.92432 \t time=119.31s\n",
      "Epoch 2/6 \t auc=0.93210 \t time=118.92s\n",
      "Epoch 3/6 \t auc=0.93305 \t time=119.68s\n",
      "Epoch 4/6 \t auc=0.93501 \t time=121.05s\n",
      "Epoch 5/6 \t auc=0.93604 \t time=119.82s\n",
      "Epoch 6/6 \t auc=0.93649 \t time=119.51s\n",
      "cv model \t auc=0.93649\n",
      "EMA model \t auc=0.93657\n",
      "Fold 5\n",
      "Epoch 1/6 \t auc=0.92201 \t time=119.35s\n",
      "Epoch 2/6 \t auc=0.93151 \t time=119.70s\n",
      "Epoch 3/6 \t auc=0.93426 \t time=120.13s\n",
      "Epoch 4/6 \t auc=0.93592 \t time=120.09s\n",
      "Epoch 5/6 \t auc=0.93671 \t time=119.16s\n",
      "Epoch 6/6 \t auc=0.93665 \t time=119.23s\n",
      "cv model \t auc=0.93665\n",
      "EMA model \t auc=0.93743\n",
      "[train] done in 63.19 min.\n"
     ]
    }
   ],
   "source": [
    "with timer('train'):\n",
    "    splits = list(\n",
    "        StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed).split(train_x, y_binary))\n",
    "    splits_test = list(KFold(n_splits=n_splits, shuffle=True, random_state=seed).split(test_x))\n",
    "\n",
    "    for fold, ((train_idx, valid_idx), (train_idx_test, _)) in enumerate(zip(splits, splits_test)):\n",
    "        print(f'Fold {fold + 1}')\n",
    "\n",
    "        x_train_fold = np.concatenate((train_x[train_idx], test_x[train_idx_test]), axis=0)\n",
    "        y_train_fold = np.concatenate((train_y[train_idx], test_y[train_idx_test]), axis=0)\n",
    "\n",
    "        x_valid_fold = train_x[valid_idx]\n",
    "        y_valid_fold = train_y[valid_idx]\n",
    "\n",
    "        valid_nan_mask = train_nan_mask[valid_idx]\n",
    "\n",
    "        y_valid_fold_binary = y_binary[valid_idx]\n",
    "        y_valid_fold_identity_binary = y_identity_binary[valid_idx]\n",
    "        evaluator = JigsawEvaluator(y_valid_fold_binary, y_valid_fold_identity_binary)\n",
    "\n",
    "        train_dataset = TextDataset(x_train_fold, targets=y_train_fold, maxlen=max_len)\n",
    "        valid_dataset = TextDataset(x_valid_fold, targets=y_valid_fold, maxlen=max_len)\n",
    "\n",
    "        train_sampler = BucketSampler(train_dataset, train_dataset.get_keys(),\n",
    "                                      bucket_size=batch_size * 20, batch_size=batch_size)\n",
    "        valid_sampler = BucketSampler(valid_dataset, valid_dataset.get_keys(),\n",
    "                                      batch_size=batch_size, shuffle_data=False)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False,\n",
    "                                  sampler=train_sampler, num_workers=0, collate_fn=collate_fn)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False,\n",
    "                                  sampler=valid_sampler, collate_fn=collate_fn)\n",
    "\n",
    "        model = NeuralNet(embedding_matrix).to(device)\n",
    "\n",
    "        ema_model = copy.deepcopy(model)\n",
    "        ema_model.eval()\n",
    "\n",
    "        ema_n = int(len(train_loader.dataset) / (updates_per_epoch * batch_size))\n",
    "        ema = EMA(model, mu, n=ema_n)\n",
    "\n",
    "        scale_fn = combine_scale_functions(\n",
    "            [partial(scale_cos, 1e-4, 5e-3), partial(scale_cos, 5e-3, 1e-3)], [0.2, 0.8])\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "        scheduler = ParamScheduler(optimizer, scale_fn, train_epochs * len(train_loader))\n",
    "\n",
    "        all_valid_preds = []\n",
    "        all_test_preds = []\n",
    "\n",
    "        for epoch in range(train_epochs):\n",
    "            start_time = time.time()\n",
    "            model.train()\n",
    "\n",
    "            for _, x_batch, y_batch in train_loader:\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                scheduler.batch_step()\n",
    "                y_pred = model(x_batch)\n",
    "\n",
    "                loss = nn.BCEWithLogitsLoss(weight=y_batch[:, 1])(y_pred[:, 0], y_batch[:, 0])\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                ema.on_batch_end(model)\n",
    "\n",
    "            valid_preds = eval_model(model, valid_loader)\n",
    "            valid_preds[valid_nan_mask] = nan_pred\n",
    "            all_valid_preds.append(valid_preds)\n",
    "\n",
    "            auc_score = evaluator.get_final_metric(valid_preds)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('Epoch {}/{} \\t auc={:.5f} \\t time={:.2f}s'.format(\n",
    "                epoch + 1, train_epochs, auc_score, elapsed_time))\n",
    "\n",
    "            test_preds = eval_model(model, test_loader)\n",
    "            all_test_preds.append(test_preds)\n",
    "\n",
    "            models[f'model_{fold}{epoch}'] = model.state_dict()\n",
    "\n",
    "            ema.on_epoch_end(model)\n",
    "\n",
    "        ema.set_weights(ema_model)\n",
    "        ema_model.lstm.flatten_parameters()\n",
    "        ema_model.gru.flatten_parameters()\n",
    "\n",
    "        models[f'ema_model_{fold}'] = ema_model.state_dict()\n",
    "\n",
    "        checkpoint_weights = np.array([2 ** epoch for epoch in range(train_epochs)])\n",
    "        checkpoint_weights = checkpoint_weights / checkpoint_weights.sum()\n",
    "\n",
    "        valid_preds_fold = np.average(all_valid_preds, weights=checkpoint_weights, axis=0)\n",
    "        valid_preds_fold[valid_nan_mask] = nan_pred\n",
    "        auc_score = evaluator.get_final_metric(valid_preds)\n",
    "        print(f'cv model \\t auc={auc_score:.5f}')\n",
    "\n",
    "        ema_valid_preds_fold = eval_model(ema_model, valid_loader)\n",
    "        ema_valid_preds_fold[valid_nan_mask] = nan_pred\n",
    "        auc_score = evaluator.get_final_metric(ema_valid_preds_fold)\n",
    "        print(f'EMA model \\t auc={auc_score:.5f}')\n",
    "\n",
    "        train_preds[valid_idx] = valid_preds_fold\n",
    "        ema_train_preds[valid_idx] = ema_valid_preds_fold\n",
    "\n",
    "        test_preds_fold = np.average(all_test_preds, weights=checkpoint_weights, axis=0)\n",
    "        ema_test_preds_fold = eval_model(ema_model, test_loader)\n",
    "\n",
    "        test_preds += test_preds_fold / n_splits\n",
    "        ema_test_preds += ema_test_preds_fold / n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv score: 0.93647 \n",
      "EMA cv score: 0.93651 \n",
      "final prediction score: 0.93660 \n"
     ]
    }
   ],
   "source": [
    "torch.save(models, 'model.pt')\n",
    "test_preds[test_nan_mask] = nan_pred\n",
    "ema_test_preds[test_nan_mask] = nan_pred\n",
    "evaluator = JigsawEvaluator(y_binary, y_identity_binary)\n",
    "auc_score = evaluator.get_final_metric(train_preds)\n",
    "ema_auc_score = evaluator.get_final_metric(ema_train_preds)\n",
    "print(f'cv score: {auc_score:<8.5f}')\n",
    "print(f'EMA cv score: {ema_auc_score:<8.5f}')\n",
    "\n",
    "train_preds = np.mean([train_preds, ema_train_preds], axis=0)\n",
    "test_preds = np.mean([test_preds, ema_test_preds], axis=0)\n",
    "auc_score = evaluator.get_final_metric(train_preds)\n",
    "print(f'final prediction score: {auc_score:<8.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7097320</td>\n",
       "      <td>0.027976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7097321</td>\n",
       "      <td>0.141418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7097322</td>\n",
       "      <td>0.355527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7097323</td>\n",
       "      <td>0.105184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7097324</td>\n",
       "      <td>0.028936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  prediction\n",
       "0  7097320    0.027976\n",
       "1  7097321    0.141418\n",
       "2  7097322    0.355527\n",
       "3  7097323    0.105184\n",
       "4  7097324    0.028936"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(SAMPLE_SUBMISSION, index_col='id')\n",
    "submission['prediction'] = test_preds * 0.9 + test_y[:, 0] * 0.1\n",
    "submission.reset_index(drop=False, inplace=True)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
